    

%% Read the numerical train dataset for Random Forest Model Selection
%% Clear everything
clear all
close all

%read the numerical dataset for Random Forest Model selection
train_rf=readtable('training_num80.csv');

%% specify that the last column is ordinal categorical data
avalues={'unacc','acc','good','vgood'};
train_rf.acceptability=categorical(train_rf.acceptability,avalues,'Ordinal',true);

%generate table with hyperparameters and out-of-bag error for each model x

%setting a list of hyperparameters
nb_of_trees=[10 20 40 50];
max_num_splits=[100 200 50];
min_leaf_size =[ 2 3 4 5];
num_variables_to_sample=[10 5 15];

%TPL = number of trees; number of leafs (depth); number of predictors

%Hyperparameters
%-nb of trees x
% -MaxNumSplits -Maximal number of decision splits X
%-MinLeafSize - minimum number of observations per tree leaf
%-NumVariablesToSample-Number of predictors to select at random for each split

rf_initialize_hyperparams;
for trees = nb_of_trees
    for nb_predictors = num_variables_to_sample
        for leaf_size = min_leaf_size
            for splits = max_num_splits
      
%split predictors and target variables
        features=train_rf(:,1:6);
        labels=train_rf(:,7);
%set up the optimization function
  %fit models;       
   start_time=cputime;
    Mdl = TreeBagger(trees, features,labels,...
    'Method','classification',...
    'ClassNames', categorical({'acc'; 'good'; 'unacc'; 'vgood'}),...
    'MaxNumSplits', splits, ...
    'NumVariablesToSample', nb_predictors, ...
    'MinLeafSize', leaf_size, ...
    'OOBPrediction','on');
end_time=cputime;
training_time=end_time-start_time;

%OOBPredict
  [Yfit,scores] = oobPredict(Mdl);
     %predict, store OOB error for each model in a table
     
%No cross-validation needed as random forest chooses a bagged subset of input data at random
% setting NumPredictorsToSample to any value but 'all' enables TreeBagger to behave like RandomForest     
     
     %save the results into one table
     number_of_trees=[number_of_trees;trees];
     leafs=[leafs;leaf_size];
     number_of_splits=[number_of_splits;splits];
     number_of_predictors = [number_of_predictors;nb_predictors];
     accuracy=[accuracy; (1-oobError(Mdl,...
         'Mode','Ensemble'))];
     time=[time;training_time];
            end
        end
    end
end

%%Merge results into one table
rf_perfm_type_conversion;
rf_models = [number_of_trees leafs number_of_splits number_of_predictors accuracy time]
rf_models = sortrows(rf_models,5,{'descend'});
writetable(rf_models, 'rf_models.csv')

%Take the model with highest accuracy and store it separately;
best_rf_model =rf_models(1,:);
writetable(best_rf_model, 'best_rf_model.csv')


%read the numerical dataset for Random Forest Model selection
test_rf=readtable('test_num80.csv');

%Train again and predict on train set, get labels and save on file

final_training_start_time=cputime;
finalMdl =  TreeBagger(table2array(best_rf_model(1,1)), features,labels,...
    'Method','classification',...
    'ClassNames', categorical({'acc'; 'good'; 'unacc'; 'vgood'}),...
    'MaxNumSplits', table2array(best_rf_model(1,3)), ...
    'NumVariablesToSample', table2array(best_rf_model(1,4)), ...
    'MinLeafSize', table2array(best_rf_model(1,2)), ...
    'OOBPrediction','on');
final_training_end_time=cputime;
final_train_time = final_training_end_time - final_training_start_time;

view(finalMdl.Trees{1},'Mode','graph')

%final model accuracy
final_model_accuracy=(1-oobError(finalMdl,...'Mode','Ensemble'))

% Predict final Result on train set
  [Yfit,scores] = oobPredict(finalMdl);
  
% save the results of predictions for train set
  predictions = [Yfit labels];
  VarNames = {'train_predictions','target'};
  predictions.Properties.VariableNames = VarNames;
  writetable(predictions,'final_results_train_Random_Forest.csv');

% Predict Results on the test set
test_features = table2array(test_rf(:,1:6));
test_labels = table2array(test_rf(:,7));

[test_Yfit,Cost] = predict(finalMdl, test_features);
final_results = array2table([test_labels test_Yfit]);
VarNames = {'target','test_predictions'};
final_results.Properties.VariableNames = VarNames;
writetable(final_results,'final_labels_Random_Forest.csv');


%---------------------------------

%Feature importance
%NO ! Principal Component Analysis (NB used corrcoef for analysis); <-- done on the trainedRandomForest;
% 2 models; one with all and one with 3-4 parameters; 
%have an optimized model 
%NO NEED for crossv-valid sa it's done on the for loop


% Save the results against their targets in a file
%predictions= [test_labels array2table(predicted_labels)];
%writetable(predictions,'final_results_random_forest.csv');

%Further Evaluation
    %accuracy;
    %cross-entropy; 
    
    %--------------future work;
   
    %produce diagram of accuracy distribution
    
    %Confusion Matrix
   

    %Future work: with increaed computational capacity, more
    %hyperparameters could be tested;
    
%delete training and test files after process is complete 


